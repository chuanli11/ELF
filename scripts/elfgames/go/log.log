Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0]
PyTorch version: 1.0.1.post2
CUDA version 10.0.130
Conda env: 
[2019-04-16 13:34:50.999] [rlpytorch.model_loader.load_env0] [info] Loading env
<module 'elfgames.go.game' from '/home/ubuntu/git/ELF/src_py/elfgames/go/game.py'> elfgames.go.game
<module 'elfgames.go.df_model3' from '/home/ubuntu/git/ELF/src_py/elfgames/go/df_model3.py'> elfgames.go.df_model3
[2019-04-16 13:34:51.006] [rlpytorch.model_loader.load_env0] [info] Parsed options: {'T': 1,
 'actor_only': False,
 'adam_eps': 0.001,
 'additional_labels': [],
 'backprop': True,
 'batchsize': 2048,
 'batchsize2': -1,
 'black_use_policy_network_only': False,
 'bn': True,
 'bn_eps': 1e-05,
 'bn_momentum': 0.0,
 'cheat_eval_new_model_wins_half': False,
 'cheat_selfplay_random_result': False,
 'check_loaded_options': True,
 'client_max_delay_sec': 1200,
 'comment': '',
 'data_aug': -1,
 'dim': 256,
 'dist_rank': -1,
 'dist_url': '',
 'dist_world_size': -1,
 'dump_record_prefix': '',
 'epsilon': 0.0,
 'eval_model_pair': '',
 'eval_num_games': 0,
 'eval_old_model': -1,
 'eval_winrate_thres': 0.55,
 'expected_num_clients': 496,
 'following_pass': False,
 'freq_update': 1,
 'gpu': 0,
 'keep_prev_selfplay': True,
 'keys_in_reply': ['V'],
 'latest_symlink': 'latest',
 'leaky_relu': False,
 'list_files': [],
 'load': '',
 'load_model_sleep_interval': 0.0,
 'loglevel': 'info',
 'lr': 0.01,
 'mcts_alpha': 0.03,
 'mcts_epsilon': 0.25,
 'mcts_persistent_tree': True,
 'mcts_pick_method': 'most_visited',
 'mcts_puct': 0.85,
 'mcts_rollout_per_batch': 1,
 'mcts_rollout_per_thread': 200,
 'mcts_root_unexplored_q_zero': False,
 'mcts_threads': 8,
 'mcts_unexplored_q_zero': False,
 'mcts_use_prior': True,
 'mcts_verbose': False,
 'mcts_verbose_time': False,
 'mcts_virtual_loss': 5,
 'mode': 'train',
 'momentum': 0.9,
 'move_cutoff': -1,
 'num_block': 20,
 'num_cooldown': 50,
 'num_episode': 1000000,
 'num_future_actions': 1,
 'num_games': 2048,
 'num_games_per_thread': -1,
 'num_minibatch': 1000,
 'num_reader': 50,
 'num_reset_ranking': 5000,
 'omit_keys': [],
 'onload': [],
 'opt_method': 'sgd',
 'parameter_print': True,
 'parsed_args': ['./train.py',
                 '--mode',
                 'train',
                 '--batchsize',
                 '2048',
                 '--num_games',
                 '2048',
                 '--keys_in_reply',
                 'V',
                 '--T',
                 '1',
                 '--use_data_parallel',
                 '--num_minibatch',
                 '1000',
                 '--num_episode',
                 '1000000',
                 '--mcts_threads',
                 '8',
                 '--mcts_rollout_per_thread',
                 '200',
                 '--keep_prev_selfplay',
                 '--keep_prev_selfplay',
                 '--use_mcts',
                 '--use_mcts_ai2',
                 '--mcts_persistent_tree',
                 '--mcts_use_prior',
                 '--mcts_virtual_loss',
                 '5',
                 '--mcts_epsilon',
                 '0.25',
                 '--mcts_alpha',
                 '0.03',
                 '--mcts_puct',
                 '0.85',
                 '--resign_thres',
                 '0.01',
                 '--gpu',
                 '0',
                 '--server_id',
                 'myserver',
                 '--eval_num_games',
                 '400',
                 '--eval_winrate_thres',
                 '0.55',
                 '--port',
                 '1234',
                 '--q_min_size',
                 '200',
                 '--q_max_size',
                 '4000',
                 '--save_first',
                 '--num_block',
                 '20',
                 '--dim',
                 '256',
                 '--weight_decay',
                 '0.0002',
                 '--opt_method',
                 'sgd',
                 '--bn_momentum=0',
                 '--num_cooldown=50',
                 '--expected_num_client',
                 '496',
                 '--selfplay_init_num',
                 '0',
                 '--selfplay_update_num',
                 '0',
                 '--eval_num_games',
                 '0',
                 '--selfplay_async',
                 '--lr',
                 '0.01',
                 '--momentum',
                 '0.9'],
 'ply_pass_enabled': 0,
 'policy_distri_cutoff': 0,
 'policy_distri_training_for_all': False,
 'port': 1234,
 'preload_sgf': '',
 'preload_sgf_move_to': -1,
 'print_result': False,
 'q_max_size': 4000,
 'q_min_size': 200,
 'ratio_pre_moves': 0,
 'record_dir': './record',
 'replace_prefix': [],
 'resign_thres': 0.01,
 'sample_nodes': ['pi,a'],
 'sample_policy': 'epsilon-greedy',
 'save_dir': './myserver',
 'save_first': True,
 'save_prefix': 'save',
 'selfplay_async': True,
 'selfplay_init_num': 0,
 'selfplay_timeout_usec': 0,
 'selfplay_update_num': 0,
 'server_addr': '',
 'server_id': 'myserver',
 'start_ratio_pre_moves': 0.5,
 'store_greedy': False,
 'suicide_after_n_games': -1,
 'tqdm': False,
 'trainer_stats': '',
 'use_data_parallel': True,
 'use_data_parallel_distributed': False,
 'use_df_feature': False,
 'use_fp16': False,
 'use_mcts': True,
 'use_mcts_ai2': True,
 'verbose': False,
 'weight_decay': 0.0002,
 'white_mcts_rollout_per_batch': -1,
 'white_mcts_rollout_per_thread': -1,
 'white_puct': -1.0,
 'white_use_policy_network_only': False}
Stats: Name  is not known!
[2019-04-16 13:34:51.007] [rlpytorch.model_loader.load_env0] [info] Finished loading env
[2019-04-16 13:34:51.007] [elf::legacy::ContextOptions-0] [info] JobId: local
[2019-04-16 13:34:51.007] [elf::legacy::ContextOptions-0] [info] #Game: 2048
[2019-04-16 13:34:51.007] [elf::legacy::ContextOptions-0] [info] T: 1
[2019-04-16 13:34:51.007] [elf::legacy::ContextOptions-0] [info] [#th=8][rl=200][per=1][eps=0.25][alpha=0.03][prior=1][c_puct=0.85][uqz=0][r_uqz=0]
[2019-04-16 13:34:51.007] [elfgames::go::train::TrainCtrl-11] [info] Finished initializing replay_buffer #Queue: 50, spec: ReaderQueue: Queue [min=200][max=4000], Length: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, Total: 0, MinSizeSatisfied: 0
[2019-04-16 13:34:51.011] [elfgames::go::train::DataOnlineLoader-17] [info] ZMQVer: 4.2.5 Reader[db=data-1555446891.db] [local] Connect to [127.0.0.1]:1234, ipv6: True, verbose: False
[2019-04-16 13:34:51.012] [elf::distributed::Reader-21] [info] Tue Apr 16 13:34:51 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
**** Options ****
Seed: 0
Time signature: 190416-133451
Client max delay in sec: 1200
#FutureActions: 1
#GamePerThread: -1
mode: train
Selfplay init min #games: 0, update #games: 0, async: True
UseMCTS: True
Data Aug: -1
Start_ratio_pre_moves: 0.5
ratio_pre_moves: 0
MoveCutOff: -1
Use DF feature: False
PolicyDistriCutOff: 0
Expected #client: 496
Server_addr: [127.0.0.1], server_id: myserver, port: 1234
#Reader: 50, Qmin_sz: 200, Qmax_sz: 4000
Verbose: False
Policy distri training for all moves: False
Min Ply from which pass is enabled: 0
Reset move ranking after 5000 actions
Resign Threshold: 0.01, Dynamic Resign Threshold, resign_prob_never: 0.1, target_fp_rate: 0.05, bounded within [1e-09, 0.5]

Komi: 7.5

*****************
Version:  690b515bb654fe80b0e9f51d13b470994182a225_staged
Mode:  train
Num Actions:  362
train: {'input': ['s', 'offline_a', 'winner', 'mcts_scores', 'move_idx', 'selfplay_ver'], 'reply': None}
SharedMem: "train", keys: ['move_idx', 'offline_a', 'winner', 'selfplay_ver', 's', 'mcts_scores']
move_idx int32_t [2048]
offline_a int64_t [2048, 1]
winner float [2048]
selfplay_ver int64_t [2048]
s float [2048, 18, 19, 19]
mcts_scores float [2048, 362]
move_idx int32_t [2048]
offline_a int64_t [2048, 1]
winner float [2048]
selfplay_ver int64_t [2048]
s float [2048, 18, 19, 19]
mcts_scores float [2048, 362]
train_ctrl: {'input': ['selfplay_ver'], 'reply': None, 'batchsize': 1}
SharedMem: "train_ctrl", keys: ['selfplay_ver']
selfplay_ver int64_t [2048]
selfplay_ver int64_t [2048]
/home/ubuntu/git/ELF/venv-elf/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:25: UserWarning: 
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
[2019-04-16 13:34:56.429] [elfgames::go::train::ThreadedCtrl-13] [info] Setting init version: 0
[2019-04-16 13:34:56.429] [elfgames::go::train::EvalSubCtrl-15] [info] Set new baseline model, ver: 0
[2019-04-16 13:34:56.429] [elfgames::go::train::SelfPlaySubCtrl-14] [info] SelfPlay: -1 -> 0
Root: "./myserver"
Keep prev_selfplay: True
Save first: 
Save to ./myserver
Filename = ./myserver/save-0.bin
About to wait for sufficient selfplay
[2019-04-16 13:34:56.710] [elfgames::go::train::ThreadedCtrl-13] [info] Tue Apr 16 13:34:56 2019, Sufficient sample for model 0
[2019-04-16 13:35:01.012] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:01 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:35:11.012] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:11 2019 Ctrl from local-lambda-quad-e89-897c-2304-1756[1]: 1555446902
[2019-04-16 13:35:11.013] [elfgames::go::train::TrainCtrl-11] [info] New allocated: local-lambda-quad-e89-897c-2304-1756, Clients[1][#max_eval=-1][#max_th=2048][#client_delay=1200], SelfplayOnly[1/100%], EvalThenSelfplay[0/0%]
[2019-04-16 13:35:11.013] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:11 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:35:21.013] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:21 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:35:31.013] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:31 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:35:41.013] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:41 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:35:51.013] [elf::distributed::Reader-21] [info] Tue Apr 16 13:35:51 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:01.014] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:01 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:11.014] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:11 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:21.014] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:21 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:31.014] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:31 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:41.014] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:41 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:36:51.015] [elf::distributed::Reader-21] [info] Tue Apr 16 13:36:51 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:37:01.015] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:01 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:37:11.015] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:11 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:37:21.015] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:21 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:37:31.016] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:31 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0]
PyTorch version: 1.0.1.post2
CUDA version 10.0.130
Conda env: 
[2019-04-16 13:37:41.270] [rlpytorch.model_loader.load_env0] [info] Loading env
<module 'elfgames.go.game' from '/home/ubuntu/git/ELF/src_py/elfgames/go/game.py'> elfgames.go.game
<module 'elfgames.go.df_model3' from '/home/ubuntu/git/ELF/src_py/elfgames/go/df_model3.py'> elfgames.go.df_model3
[2019-04-16 13:37:41.277] [rlpytorch.model_loader.load_env0] [info] Parsed options: {'T': 1,
 'actor_only': False,
 'adam_eps': 0.001,
 'additional_labels': [],
 'backprop': True,
 'batchsize': 2048,
 'batchsize2': -1,
 'black_use_policy_network_only': False,
 'bn': True,
 'bn_eps': 1e-05,
 'bn_momentum': 0.0,
 'cheat_eval_new_model_wins_half': False,
 'cheat_selfplay_random_result': False,
 'check_loaded_options': True,
 'client_max_delay_sec': 1200,
 'comment': '',
 'data_aug': -1,
 'dim': 256,
 'dist_rank': -1,
 'dist_url': '',
 'dist_world_size': -1,
 'dump_record_prefix': '',
 'epsilon': 0.0,
 'eval_model_pair': '',
 'eval_num_games': 0,
 'eval_old_model': -1,
 'eval_winrate_thres': 0.55,
 'expected_num_clients': 496,
 'following_pass': False,
 'freq_update': 1,
 'gpu': 0,
 'keep_prev_selfplay': True,
 'keys_in_reply': ['V'],
 'latest_symlink': 'latest',
 'leaky_relu': False,
 'list_files': [],
 'load': '',
 'load_model_sleep_interval': 0.0,
 'loglevel': 'info',
 'lr': 0.01,
 'mcts_alpha': 0.03,
 'mcts_epsilon': 0.25,
 'mcts_persistent_tree': True,
 'mcts_pick_method': 'most_visited',
 'mcts_puct': 0.85,
 'mcts_rollout_per_batch': 1,
 'mcts_rollout_per_thread': 200,
 'mcts_root_unexplored_q_zero': False,
 'mcts_threads': 8,
 'mcts_unexplored_q_zero': False,
 'mcts_use_prior': True,
 'mcts_verbose': False,
 'mcts_verbose_time': False,
 'mcts_virtual_loss': 5,
 'mode': 'train',
 'momentum': 0.9,
 'move_cutoff': -1,
 'num_block': 20,
 'num_cooldown': 50,
 'num_episode': 1000000,
 'num_future_actions': 1,
 'num_games': 2048,
 'num_games_per_thread': -1,
 'num_minibatch': 1000,
 'num_reader': 50,
 'num_reset_ranking': 5000,
 'omit_keys': [],
 'onload': [],
 'opt_method': 'sgd',
 'parameter_print': True,
 'parsed_args': ['./train.py',
                 '--mode',
                 'train',
                 '--batchsize',
                 '2048',
                 '--num_games',
                 '2048',
                 '--keys_in_reply',
                 'V',
                 '--T',
                 '1',
                 '--use_data_parallel',
                 '--num_minibatch',
                 '1000',
                 '--num_episode',
                 '1000000',
                 '--mcts_threads',
                 '8',
                 '--mcts_rollout_per_thread',
                 '200',
                 '--keep_prev_selfplay',
                 '--keep_prev_selfplay',
                 '--use_mcts',
                 '--use_mcts_ai2',
                 '--mcts_persistent_tree',
                 '--mcts_use_prior',
                 '--mcts_virtual_loss',
                 '5',
                 '--mcts_epsilon',
                 '0.25',
                 '--mcts_alpha',
                 '0.03',
                 '--mcts_puct',
                 '0.85',
                 '--resign_thres',
                 '0.01',
                 '--gpu',
                 '0',
                 '--server_id',
                 'myserver',
                 '--eval_num_games',
                 '400',
                 '--eval_winrate_thres',
                 '0.55',
                 '--port',
                 '1234',
                 '--q_min_size',
                 '200',
                 '--q_max_size',
                 '4000',
                 '--save_first',
                 '--num_block',
                 '20',
                 '--dim',
                 '256',
                 '--weight_decay',
                 '0.0002',
                 '--opt_method',
                 'sgd',
                 '--bn_momentum=0',
                 '--num_cooldown=50',
                 '--expected_num_client',
                 '496',
                 '--selfplay_init_num',
                 '0',
                 '--selfplay_update_num',
                 '0',
                 '--eval_num_games',
                 '0',
                 '--selfplay_async',
                 '--lr',
                 '0.01',
                 '--momentum',
                 '0.9'],
 'ply_pass_enabled': 0,
 'policy_distri_cutoff': 0,
 'policy_distri_training_for_all': False,
 'port': 1234,
 'preload_sgf': '',
 'preload_sgf_move_to': -1,
 'print_result': False,
 'q_max_size': 4000,
 'q_min_size': 200,
 'ratio_pre_moves': 0,
 'record_dir': './record',
 'replace_prefix': [],
 'resign_thres': 0.01,
 'sample_nodes': ['pi,a'],
 'sample_policy': 'epsilon-greedy',
 'save_dir': './myserver',
 'save_first': True,
 'save_prefix': 'save',
 'selfplay_async': True,
 'selfplay_init_num': 0,
 'selfplay_timeout_usec': 0,
 'selfplay_update_num': 0,
 'server_addr': '',
 'server_id': 'myserver',
 'start_ratio_pre_moves': 0.5,
 'store_greedy': False,
 'suicide_after_n_games': -1,
 'tqdm': False,
 'trainer_stats': '',
 'use_data_parallel': True,
 'use_data_parallel_distributed': False,
 'use_df_feature': False,
 'use_fp16': False,
 'use_mcts': True,
 'use_mcts_ai2': True,
 'verbose': False,
 'weight_decay': 0.0002,
 'white_mcts_rollout_per_batch': -1,
 'white_mcts_rollout_per_thread': -1,
 'white_puct': -1.0,
 'white_use_policy_network_only': False}
Stats: Name  is not known!
[2019-04-16 13:37:41.278] [rlpytorch.model_loader.load_env0] [info] Finished loading env
[2019-04-16 13:37:41.278] [elf::legacy::ContextOptions-0] [info] JobId: local
[2019-04-16 13:37:41.278] [elf::legacy::ContextOptions-0] [info] #Game: 2048
[2019-04-16 13:37:41.278] [elf::legacy::ContextOptions-0] [info] T: 1
[2019-04-16 13:37:41.278] [elf::legacy::ContextOptions-0] [info] [#th=8][rl=200][per=1][eps=0.25][alpha=0.03][prior=1][c_puct=0.85][uqz=0][r_uqz=0]
[2019-04-16 13:37:41.278] [elfgames::go::train::TrainCtrl-11] [info] Finished initializing replay_buffer #Queue: 50, spec: ReaderQueue: Queue [min=200][max=4000], Length: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, Total: 0, MinSizeSatisfied: 0
[2019-04-16 13:37:41.279] [elfgames::go::train::DataOnlineLoader-17] [info] ZMQVer: 4.2.5 Reader[db=data-1555447061.db] [local] Connect to [127.0.0.1]:1234, ipv6: True, verbose: False
[2019-04-16 13:37:41.279] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:41 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
**** Options ****
Seed: 0
Time signature: 190416-133741
Client max delay in sec: 1200
#FutureActions: 1
#GamePerThread: -1
mode: train
Selfplay init min #games: 0, update #games: 0, async: True
UseMCTS: True
Data Aug: -1
Start_ratio_pre_moves: 0.5
ratio_pre_moves: 0
MoveCutOff: -1
Use DF feature: False
PolicyDistriCutOff: 0
Expected #client: 496
Server_addr: [127.0.0.1], server_id: myserver, port: 1234
#Reader: 50, Qmin_sz: 200, Qmax_sz: 4000
Verbose: False
Policy distri training for all moves: False
Min Ply from which pass is enabled: 0
Reset move ranking after 5000 actions
Resign Threshold: 0.01, Dynamic Resign Threshold, resign_prob_never: 0.1, target_fp_rate: 0.05, bounded within [1e-09, 0.5]

Komi: 7.5

*****************
Version:  690b515bb654fe80b0e9f51d13b470994182a225_staged
Mode:  train
Num Actions:  362
train: {'input': ['s', 'offline_a', 'winner', 'mcts_scores', 'move_idx', 'selfplay_ver'], 'reply': None}
SharedMem: "train", keys: ['winner', 'move_idx', 'offline_a', 'selfplay_ver', 'mcts_scores', 's']
winner float [2048]
move_idx int32_t [2048]
offline_a int64_t [2048, 1]
selfplay_ver int64_t [2048]
mcts_scores float [2048, 362]
s float [2048, 18, 19, 19]
winner float [2048]
move_idx int32_t [2048]
offline_a int64_t [2048, 1]
selfplay_ver int64_t [2048]
mcts_scores float [2048, 362]
s float [2048, 18, 19, 19]
train_ctrl: {'input': ['selfplay_ver'], 'reply': None, 'batchsize': 1}
SharedMem: "train_ctrl", keys: ['selfplay_ver']
selfplay_ver int64_t [2048]
selfplay_ver int64_t [2048]
/home/ubuntu/git/ELF/venv-elf/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:25: UserWarning: 
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
[2019-04-16 13:37:46.511] [elfgames::go::train::ThreadedCtrl-13] [info] Setting init version: 0
[2019-04-16 13:37:46.511] [elfgames::go::train::EvalSubCtrl-15] [info] Set new baseline model, ver: 0
[2019-04-16 13:37:46.511] [elfgames::go::train::SelfPlaySubCtrl-14] [info] SelfPlay: -1 -> 0
Root: "./myserver"
Keep prev_selfplay: True
Save first: 
Save to ./myserver
Filename = ./myserver/save-0.bin
About to wait for sufficient selfplay
[2019-04-16 13:37:46.764] [elfgames::go::train::ThreadedCtrl-13] [info] Tue Apr 16 13:37:46 2019, Sufficient sample for model 0
[2019-04-16 13:37:51.279] [elf::distributed::Reader-21] [info] Tue Apr 16 13:37:51 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:38:01.280] [elf::distributed::Reader-21] [info] Tue Apr 16 13:38:01 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:38:11.280] [elf::distributed::Reader-21] [info] Tue Apr 16 13:38:11 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:38:21.280] [elf::distributed::Reader-21] [info] Tue Apr 16 13:38:21 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:38:31.280] [elf::distributed::Reader-21] [info] Tue Apr 16 13:38:31 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
[2019-04-16 13:38:41.280] [elf::distributed::Reader-21] [info] Tue Apr 16 13:38:41 2019, Reader: no message, Stats: 0/0/0, wait for 10 sec ... 
